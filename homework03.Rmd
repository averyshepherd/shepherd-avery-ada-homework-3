---
title: "Homework 03"
author: "Avery Shepherd"
date: "4/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(infer)
library(simpleboot)
```

```{r}
f <- "https://raw.githubusercontent.com/difiore/ada-2021-datasets/main/KamilarAndCooperData.csv"
d <- read_csv(f, col_names = T)
glimpse(d)
```


## Chalenge 1

### Linear model
Fit weaning age ~ brain size regression model and produce a scatterplot with the fitted line superimposed upon the data
```{r}
lin_lm <- lm(data = d, WeaningAge_d ~ Brain_Size_Species_Mean)
lin_lm.intercept = coef(lin_lm)[[1]]
lin_lm.slope = coef(lin_lm)[[2]]
ggplot(d, aes(x = Brain_Size_Species_Mean, y = WeaningAge_d)) +
  geom_point() + 
  geom_smooth(method = "lm", se = F) + 
  annotate(geom = "text", x = 400, y = 250, label = paste0("y = ", round(lin_lm.intercept, 2), " + " , round(lin_lm.slope, 2), "x"))
```

Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0:β1=0,HA:β1≠0. Also, find a 90% CI for the slope (β1) parameter.
```{r}
summary(lin_lm)
ci <- confint(lin_lm, level = .9)
```
The point slope estimate is 2.6371, meaning on average, if age goes up 2.6371 years, brain size will increase by one. We also reject the null hypothesis, meaning there is a significant relationship between brain size and age (p < 2e-16).


Using your model, add lines for the 90% confidence and prediction interval bands on the plot, and add a legend to differentiate between the lines
```{r}
ggplot(d, aes(x = Brain_Size_Species_Mean, y = WeaningAge_d)) +
  geom_point() + 
  geom_smooth(method = "lm", level = .9) + 
  annotate(geom = "text", 
           x = 400, y = 250, 
           label = paste0("y = ", round(lin_lm.intercept, 2), " + " , round(lin_lm.slope, 2), "x"))
```

Produce a point estimate and associated 90% prediction interval for the weaning age of a species whose brain weight is 750 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?
```{r}
predict(lin_lm, newdata = data.frame(Brain_Size_Species_Mean = 750), 
        interval = "predict", 
        level = .9)
```
I would not necessarily trust the model to predict the value accurately because of how large the value is. The value is an outlier compared to the val?preues that the model was made from.

### Logged Model

Fit log(weaning age) ~ log(brain size) regression model and produce a scatterplot with the fitted line superimposed upon the data
```{r}
log_lm <- lm(data = d, log(WeaningAge_d) ~ log(Brain_Size_Species_Mean))
log_lm.intercept = coef(log_lm)[[1]]
log_lm.slope = coef(log_lm)[[2]]
ggplot(d, aes(x = log(Brain_Size_Species_Mean), y = log(WeaningAge_d))) +
  geom_point() + 
  geom_smooth(method = "lm", se = F) + 
  annotate(geom = "text", x = 5, y = 5, label = paste0("y = ", round(log_lm.intercept, 2), " + " , round(log_lm.slope, 2), "x"))
```

Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0:β1=0,HA:β1≠0. Also, find a 90% CI for the slope (β1) parameter.
```{r}
summary(log_lm)
ci <- confint(log_lm, level = .9)
```
The point slope estimate is 0.57116, meaning on average, if age goes up 0.57116 years, brain size will increase by one. We also reject the null hypothesis, meaning there is a significant relationship between brain size and age (p < 2e-16).

Using your model, add lines for the 90% confidence and prediction interval bands on the plot, and add a legend to differentiate between the lines
```{r}
ggplot(d, aes(x = log(Brain_Size_Species_Mean), y = log(WeaningAge_d))) +
  geom_point() + 
  geom_smooth(method = "lm", level = .9) + 
  annotate(geom = "text", 
           x = 5, y = 5, 
           label = paste0("y = ", round(log_lm.intercept, 2), " + " , round(log_lm.slope, 2), "x"))
```

Produce a point estimate and associated 90% prediction interval for the weaning age of a species whose brain weight is 750 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?
```{r}
predict(lin_lm, newdata = data.frame(Brain_Size_Species_Mean = log(750)), 
        interval = "predict", 
        level = .9)
```
I would trust this model more as the log value of 750 is much closer the values the model was made on.


Looking at your two models (i.e., untransformed versus log-log transformed), which do you think is better? Why?
*I think that the log transformed model is better because when the values are logged it becomes a much more linear relationship, meaning the slope is more accurate for the values.*


## Challenge 2

Using the “KamilarAndCooperData.csv” dataset, run a linear regression looking at log(MeanGroupSize) in relation to log(Body_mass_female_mean) and report your β coeffiecients (slope and intercept).
```{r}
d$logMeanGroupSize <- log(d$MeanGroupSize)
d$logBody_mass_female_mean <- log(d$Body_mass_female_mean)
c2lm <- lm(data = d, logMeanGroupSize ~ logBody_mass_female_mean)
summary(c2lm)
```
β0 is -1.7772580 and β1 is 0.5062813 

Then, use bootstrapping to sample from the dataset 1000 times with replacement, each time fitting the same model and calculating the appropriate coefficients. This generates a bootstrap sampling distribution for each β coefficient. Plot a histogram of these sampling distributions for β0 and β1.
```{r}
boot_lm <- lm.boot(c2lm, 1000, rows = T)
boot_coefs <- data.frame(t(samples(boot_lm, "coef")))
ggplot(boot_coefs, aes(x = X.Intercept.)) + geom_histogram(bins = 50)
ggplot(boot_coefs, aes(x = logBody_mass_female_mean)) + geom_histogram(bins = 50)
```

Estimate the standard error for each of your  
β coefficients as the standard deviation of the sampling distribution from your bootstrap.
```{r}
boot_coefs$SE <- (samples(boot_lm, "rss") / 211) / sqrt(samples(boot_lm, "rss"))
mean(boot_coefs$SE)
```

Also determine the 95% CI for each of your β coefficients based on the appropriate quantiles from your sampling distribution
```{r}
boot_coefs$upperCI <- boot_coefs$SE + boot_coefs$logBody_mass_female_mean
boot_coefs$lowerCI <- boot_coefs$SE - boot_coefs$logBody_mass_female_mean
# upper 
mean(boot_coefs$upperCI)
mean(boot_coefs$lowerCI)
```

How do the SEs estimated from the bootstrap sampling distribution compare to those estimated mathematically as part of lm() function?
```{r}
coef(summary(c2lm))[, "Std. Error"]
```
The SE for the bootstrap sampling distribution has a lower SE.

How do your bootstrap CIs compare to those estimated mathematically as part of the lm() function?
```{r}
confint(c2lm, level = .95)
```
The confidence interval is smaller for bootstrap


## Challenge 3
Write your own function, called boot_lm(), that takes as its arguments a dataframe (d=), a linear model (model=, written as a character string, e.g., “logGS ~ logBM”), a user-defined confidence interval level (conf.level=, with default “0.95”), and a number of bootstrap replicates (reps=, with default “1000”).
Your function should return a dataframe that includes: the β coefficient names (β0, β1, etc.); the value of the β coefficients, their standard errors, and their upper and lower CI limits for the linear model based on your original dataset; and the mean β coefficient estimates, SEs, and CI limits for those coefficients based on your bootstrap.
```{r}
library(boot)

boot_lm <- function(d, model, conf.level=.95, reps=1000) {
  form = as.formula(model)
  results <- data.frame(beta0 = numeric(),
                        beta1 = numeric(),
                        beta0SE = numeric(),
                        beta1SE = numeric(),
                        beta0low = numeric(),
                        beta0upp = numeric(),
                        beta1low = numeric(),
                        beta1upp = numeric())
  for (i in  1:reps) { 
    data <- d[sample(x = 1:length(d), size = length(d), replace = TRUE),]
    mod <- lm(form, data)
    coefs <- coef(summary(mod))
    confs <- confint(mod)
    res <- data.frame(beta0 = coefs[1, "Estimate"], 
                      beta1 = coefs[2, "Estimate"], 
                      beta0SE = coefs[1, "Std. Error"],
                      beta1SE = coefs[2, "Std. Error"],
                      beta0low = confs[1,1], 
                      beta0upp = confs[1,2],
                      beta1low = confs[2,1],
                      beta1upp = confs[2,2])
    if (nrow(coefs) == 3) {
      res <- cbind(res, data.frame(beta2 = coefs[3, "Estimate"], 
                      beta2SE = coefs[3, "Std. Error"],
                      beta2low = confs[3,1], 
                      beta2upp = confs[3,2]))
    }
    results <- rbind(results, res)
  }
  
  results$meanbeta0 <- mean(results$beta0)
  results$meanbeta1 <- mean(results$beta1)
  results$meanbeta0SE <- mean(results$beta0SE)
  results$meanbeta1SE <- mean(results$beta1SE)
  results$meanbeta0low <- mean(results$beta0low)
  results$meanbeta0upp <- mean(results$beta0upp)
  results$meanbeta1low <- mean(results$beta1low)
  results$meanbeta1upp <- mean(results$beta1upp)
  if (nrow(coefs) == 3) {
    results$meanbeta2 <- mean(results$beta2)
    results$meanbeta2SE <- mean(results$beta2SE)
    results$meanbeta2low <- mean(results$beta2low)
    results$meanbeta2upp <- mean(results$beta2upp)
  }
  return(results)
}
```

Use your function to run the following models on the “KamilarAndCooperData.csv” dataset:
log(MeanGroupSize) ~ log(Body_mass_female_mean)
```{r}
r <- boot_lm(d, "log(MeanGroupSize) ~ log(Body_mass_female_mean)")
r
```

log(DayLength_km) ~ log(Body_mass_female_mean)
```{r}
r <- boot_lm(d, "log(DayLength_km) ~ log(Body_mass_female_mean)")
r
```

log(DayLength_km) ~ log(Body_mass_female_mean) + log(MeanGroupSize)
```{r}
r <- boot_lm(d, "log(DayLength_km) ~ log(Body_mass_female_mean) + log(MeanGroupSize)")
r
```

